{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWReEGBK7Cc3DztVeX1HHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codewithjaspreet/NLP/blob/master/Naive_Bayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UeVRtLh7oJQE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTcNCyeeoLgb",
        "outputId": "fcd21bd7-6935-45c0-a434-05cab8dd58ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt = twitter_samples.strings('positive_tweets.json')\n",
        "nt = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "qUDL07JPoOEb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "rand = np.random.randint(0 , 5000 , 5)"
      ],
      "metadata": {
        "id": "JshDjZy4oZZR"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Positive Tweets ::')\n",
        "print('\\033[92m')\n",
        "for i in rand:\n",
        "  print(pt[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xgjfr3Kol3T",
        "outputId": "bc2d50c1-25d8-43f9-a51d-0c2c022f3fa5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Tweets ::\n",
            "\u001b[92m\n",
            "@ZarlashtFaisal @Tabinda_Samar Sethi was HIGH ??? :)\n",
            "Fav if awake fam :)\n",
            "Just smile even your in Pain :) http://t.co/AxTiqf0xek\n",
            "camillus pleaseee? :)\n",
            "Why have i just painted my nails pink :) ???\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Negative Tweets ::')\n",
        "print('\\033[92m')\n",
        "for i in rand:\n",
        "  print(nt[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTmc5tejo84Y",
        "outputId": "3066c7d7-ce99-46d9-806c-58ec31ae28bb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Tweets ::\n",
            "\u001b[92m\n",
            "I need a big cuddle from Lew and kisses on my face :(((( I don't want to go through this again\n",
            "@kaiality too late now :(\n",
            "traffic :-(\n",
            "Soft defence by the best defensive team there :( #NRLTigersRoosters\n",
            "@LukeBryanOnline Yayyyy!!! I hope it's not while I am knocked out by anesthesia. I will be so sad if I miss it :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def processTweet(tweet):\n",
        "    stemmer  = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "\n",
        "    # remove stock market tickers like $GE\n",
        "\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "    # remove old style retweet text \"RT\"\n",
        "\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # remove hyperlinks\n",
        "\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "    # remove hashtags\n",
        "\n",
        "    # only removing the hash # sign from the word\n",
        "\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "    # tokenize tweets\n",
        "\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "\n",
        "                                 reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "\n",
        "    # remove stopwords, and punctuations\n",
        "\n",
        "    for wrd in tweet_tokens:\n",
        "        if (wrd not in stopwords_english and  # remove stopwords\n",
        "                wrd not in string.punctuation):\n",
        "\n",
        "               stem  = stemmer.stem(wrd)\n",
        "               tweets_clean.append(stem)                                     \n",
        "\n",
        "    return tweets_clean"
      ],
      "metadata": {
        "id": "3uM1HGudpJn0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = pt[np.random.randint(0,5000)]\n",
        "print('Raw Tweet : \\n ' , tweet)\n",
        "tweet = processTweet(tweet)\n",
        "\n",
        "print('After Processing : \\n' , tweet )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBvRtuHVqYVe",
        "outputId": "4a200000-2e29-489e-c33b-7b0f4eaf69a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Tweet : \n",
            "  @WforWoman 5. Over 20 W kurtas! And my Mom has about half the number I have :D #WSaleLove\n",
            "After Processing : \n",
            " ['5', '20', 'w', 'kurta', 'mom', 'half', 'number', ':d', 'wsalelov']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tweets(tweets, ys):\n",
        "  ys_list = np.squeeze(ys).tolist()\n",
        "  freqs ={}\n",
        "\n",
        "  for y, tweet in zip(ys_list, tweets):\n",
        "    for word in processTweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "  \n",
        "  return freqs"
      ],
      "metadata": {
        "id": "CST6NixAq0wC"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup(freqs, word, label):\n",
        "  n = 0\n",
        "  pair = (word, label)\n",
        "  if pair in freqs:\n",
        "    n = freqs[pair]\n",
        "  return n "
      ],
      "metadata": {
        "id": "87jpLT1Er6dr"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data for training and testing \n",
        "train_pos = pt[:4000]\n",
        "test_pos = pt[4000:]\n",
        "\n",
        "train_neg = nt[:4000]\n",
        "test_neg = nt[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# numpy array for the labels in the training set\n",
        "train_y = np.append(np.ones((len(train_pos))), np.zeros((len(train_neg))))\n",
        "test_y = np.append(np.ones((len(test_neg))), np.zeros((len(test_neg))))"
      ],
      "metadata": {
        "id": "1TvitAPIsisj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a frequency dictionary\n",
        "freqs = count_tweets(train_x, train_y)\n",
        "\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "  logliklihood = {}\n",
        "  logprior = 0\n",
        "\n",
        "  # calculate V, number of unique words in the vocabulary\n",
        "  vocab = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(vocab)\n",
        "\n",
        "  ## Calculate N_pos, N_neg, V_pos, V_neg\n",
        "  # N_pos : total number of positive words\n",
        "  # N_neg : total number of negative words\n",
        "  # V_pos : total number of unique positive words\n",
        "  # V_neg : total number of unique negative words\n",
        "\n",
        "  N_pos = N_neg = V_pos = V_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1]>0:\n",
        "      V_pos +=1\n",
        "      N_pos += freqs[pair]\n",
        "    else:\n",
        "      V_neg +=1\n",
        "      N_neg += freqs[pair]\n",
        "\n",
        "  # Number of Documents (tweets)\n",
        "  D = len(train_y)\n",
        "\n",
        "  # D_pos, number of positive documnets\n",
        "  D_pos = len(list(filter(lambda x: x>0, train_y)))\n",
        "\n",
        "  # D_pos, number of negative documnets\n",
        "  D_neg = len(list(filter(lambda x: x<=0, train_y)))\n",
        "\n",
        "  # calculate the logprior\n",
        "  logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "  for word in vocab:\n",
        "    freqs_pos = lookup(freqs, word, 1)\n",
        "    freqs_neg = lookup(freqs, word, 0)\n",
        "\n",
        "    # calculte the probability of each word being positive and negative\n",
        "    p_w_pos = (freqs_pos+1)/(N_pos+V)\n",
        "    p_w_neg = (freqs_neg+1)/(N_neg+V)\n",
        "\n",
        "\n",
        "  return logprior,logliklihood\n",
        "\n"
      ],
      "metadata": {
        "id": "GzRu-lhJtPoR"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logprior ,loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myF-KDrfwCHw",
        "outputId": "3a360836-075f-48e3-aaaa-b48a25c0682e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "  word_l = processTweet(tweet)\n",
        "  p = 0\n",
        "  p+=logprior\n",
        "\n",
        "  for word in word_l:\n",
        "    if word in loglikelihood:\n",
        "      p+=loglikelihood[word]\n",
        "\n",
        "  return p"
      ],
      "metadata": {
        "id": "-HyvJwO7wGHS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "  accuracy = 0\n",
        "  y_hats = []\n",
        "  for tweet in test_x:\n",
        "    if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "      y_hat_i = 1\n",
        "    else:\n",
        "      y_hat_i = 0\n",
        "    y_hats.append(y_hat_i)\n",
        "  error = np.mean(np.absolute(test_y - y_hats))\n",
        "  accuracy = 1-error\n",
        "\n",
        "  return accuracy\n",
        "  \n",
        "  print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ],
      "metadata": {
        "id": "WQMI-Z0exv1n"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = ['I am happy' , 'I am bad', 'this movie should have been great. ', 'great', 'great great', 'great great great', 'great great great great', 'I am not happy : (' ]\n",
        "for tweet in tweets:\n",
        "  p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "\n",
        "  if p>1:\n",
        "    print ('\\033 [92m' )\n",
        "    print (f' {tweet} : : Positive sentiment ({p:.2f}) ')\n",
        "\n",
        "  else:\n",
        "    print ('\\033[91m' )\n",
        "    print (f' {tweet} : : Negative sentiment ({p:.2f}) ')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeRyiMv3x1LU",
        "outputId": "b809d8f5-8221-470c-a9fb-1c4a9949b5df"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\n",
            " I am happy : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " I am bad : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " this movie should have been great.  : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " great : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " great great : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " great great great : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " great great great great : : Negative sentiment (0.00) \n",
            "\u001b[91m\n",
            " I am not happy : ( : : Negative sentiment (0.00) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kt_6ov2TyKEG"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}